---
# Source: tgi/templates/configmap.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: ConfigMap
metadata:
  name: config-{endpoint}
data:
  MODEL_ID: "{modelName}"
  PORT: "{port}"
  HF_TOKEN: "{huggingFaceToken}"
  https_proxy: "${HTTP_PROXY}"
  no_proxy: "${NO_PROXY}"
  HABANA_LOGS: "/tmp/habana_logs"
  NUMBA_CACHE_DIR: "/tmp"
  HF_HOME: "/tmp/.cache/huggingface"
  CUDA_GRAPHS: "0"
  OLLAMA_DEBUG: "1"
  OLLAMA_KEEP_ALIVE: "-1"
---
# Source: tgi/templates/service.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: Service
metadata:
  name: "{endpoint}"
spec:
  type: ClusterIP
  ports:
    - port: 80
      targetPort: "11434"
      protocol: TCP
      name: "{endpoint}"
  selector:
    app: "{endpoint}"
---
# Source: tgi/templates/deployment.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: apps/v1
kind: Deployment
metadata:
  name: "{endpoint}"
  labels:
    app: "{endpoint}"
    app.kubernetes.io/name: ollama
    app.kubernetes.io/instance: ollama
    app.kubernetes.io/version: "0.5.13"
spec:
  replicas: 1
  selector:
    matchLabels:
      app: "{endpoint}"
  template:
    metadata:
      labels:
        app: "{endpoint}"
    spec:
      containers:
        - name: tgi
          envFrom:
            - configMapRef:
                name: config-{endpoint}
          image: "ollama/ollama:0.5.13"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 11434
              protocol: TCP
          lifecycle:
            postStart:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    while ! /bin/ollama ps > /dev/null 2>&1; do
                      sleep 5
                    done
                    # Pull models at container startup
                    echo $MODEL_ID | xargs -n1 /bin/ollama pull || echo "Failed to pull model: $MODEL_ID"
                    # Run models at container startup
                    echo $MODEL_ID | xargs -n1 /bin/ollama run || echo "Failed to run model: $MODEL_ID"
          resources:
            requests:
              memory: "16Gi"
              cpu: "2000m"
            limits:
              memory: "64Gi"
              cpu: "8000m"
          volumeMounts:
            - name: ollama-data
              mountPath: "/root/.ollama"
      volumes:
        - name: ollama-data
          emptyDir: {}